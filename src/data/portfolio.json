[{"slug":"command-center","entry":{"title":"Lingoport Command Center","thumbnail":{"url":"https://imgur.com/FNIwCkB.png","alt":"Thumbnail for the Lingoport Command Center project; the logo for Command Center; the Lingoport logo (spinning sun rays - illustrated) sits in the center of 4 rectangles, representing a desk full of computer screens with a lot to monitor."},"contents":[{"type":"paragraph","text":"Command Center is Lingoport’s continuous internationalization (i18n) and localization platform. It’s our flagship product, consolidating configuration, execution, and reporting for both Globalyzer and Localyzer projects."},{"type":"paragraph","text":"Development of Command Center started in the summer of 2022. Before that, the user experience of Lingoport’s product suite was a challenging one. Configurations and some actions happened on one system, while viewing results and other actions happened on another system. The developer experience was equally as fragmented into multiple platforms for what represented the interface to the core products, Globalyzer for i18n and Localyzer for localization."},{"type":"paragraph","text":"I am a full-stack developer on this project, however my specialty within the team is frontend development. As a result, I was tasked with creating the design for the user interface. I mapped out how the app should be organized through flow diagrams and user stories. Then I iteratively created low to high fidelity UI prototypes in Figma, discussing it with the team along the way."},{"type":"paragraph","text":"When development initially started, I was initially a bit out of my comfort zone - the tech stack was… bit older than I expected. But I focused on my fundamentals of universal design and my understanding of accessibility (a11y) best practices. Along the way I got to incorporate a strong understanding of i18n best practices into my universal design principles as well."},{"type":"paragraph","text":"After several releases to our customers, we found a need to have a much more responsive front-end. Command Center is a highly dynamic platform that interfaces multiple core products of Lingoport’s, and the static model-view-controller (MVC) nature of the old stack was once again, a point of frustration in both the developer and user experience. To combat this I set up a way for the team to slowly migrate over to a React based front-end, and we started with the pages that contained the most dynamic content and needed it the most."},{"type":"image","url":"https://imgur.com/uNjYham.png","alt":"A screenshot of the login page for Command Center; input fields to enter in your username and password sit on top of a background of a picture of Italy taken from the International Space Station."},{"type":"image","url":"https://imgur.com/tnV2Lo6.png","alt":"A screenshot of the main index page for Command Center; a list of all your set up projects takes up most of the screen."},{"type":"image","url":"https://imgur.com/gRGiPua.png","alt":"A screenshot of a project Overview page in Command Center."},{"type":"image","url":"https://imgur.com/IvUiCpO.png","alt":"A screenshot of a Globalyzer Report page in Command Center."},{"type":"image","url":"https://imgur.com/o5pD6Pc.png","alt":"A screenshot of an Issues page in Command Center."},{"type":"image","url":"https://imgur.com/rLljL3W.png","alt":"A screenshot of a Localyzer Report page in Command Center."},{"type":"image","url":"https://imgur.com/uNXEnR1.png","alt":"A screenshot of a Strings page in Command Center."},{"type":"subheading","text":"Localyzer String Manager"},{"type":"paragraph","text":"String Manager is a new Localyzer feature providing visibility into a project's externalized strings. It offers key information about string keys, values, source files, line numbers, and translation status. It also features a review and recommendation system to ensure source string quality. Localization managers using Command Center have given us the feedback that String Manager is one their favorite features of Command Center, and they can’t wait for the upgrades to it we will release."},{"type":"paragraph","text":"In addition to being a great feature for the customers, it’s also been an awesome opportunity to revisit the process of planning, designing, iterating, and implementing to create a new feature from scratch. Because of my integral involvement in the String Manager project, I got to record a walkthrough to help customers quickly familiarize themselves with the new feature if they wanted. Feel free to check it out below!"},{"type":"video","url":"https://www.youtube.com/embed/lO1THJAhXSc"},{"type":"subheading","text":"Iconography and Logo Design"},{"type":"paragraph","text":"One of my favorite side quests that I have gone on at Lingoport has been Logo design. It started out pretty innocent, but eventually it turned into me redesigning every logo the company has… oops."},{"type":"paragraph","text":"I have some graphic design experience from both my graduate and undergraduate degrees at ATLAS, and something I opted for when designing and developing Command Center was to create the icons within the application from scratch. This led to me designing the logo for Command Center as well, and subsequently new logos for the existing Globalyzer and Localyzer products as well. Eventually this expanded to even include tweaks to the company logo itself to make it more professional."},{"type":"image","url":"https://imgur.com/xXplJIY.png","alt":"The logo for Command Center; the Lingoport logo (spinning sun rays - illustrated) sits in the center of 4 rectangles, representing a desk full of computer screens with a lot to monitor."},{"type":"image","url":"https://imgur.com/kLAzmNJ.png","alt":"The logo for Globalyzer; the Lingoport logo (spinning sun rays - illustrated) sits inside a looking glass, indicating that Globalyzer will be searching for issues within your code."},{"type":"image","url":"https://imgur.com/b83ALyc.png","alt":"The logo for Localyzer; the Lingoport logo (spinning sun rays - illustrated) sits within a chat bubble, layered on top of a file folder. The chat bubble represents the translation services, the file folder represents the code base, and the layered aspect of it all represents the way that Localyzer connects the two."},{"type":"image","url":"https://imgur.com/2WyWAH7.png","alt":"The logo for LocalyzerQA; since LocalyzerQA is in the Localyzer product family, its logo is very similar to the Localyzer Logo. The difference is that the Lingoport logo is moved to the file folder to make room for a QA “check mark” icon in the chat bubble."},{"type":"image","url":"https://imgur.com/TtCJ6Jp.png","alt":"The logo for Localyzer Express; since Localyzer Express is in the Localyzer product family, its logo is very similar to the Localyzer Logo. The difference is that the Lingoport logo is moved to the file folder to make room for a translation symbol in the chat bubble."},{"type":"image","url":"https://imgur.com/AiiKAT6.png","alt":"The redesigned Lingoport logo. The words Lingoport are written in blue, with green spinning sun rays replacing the dot in the “i” in Lingoport."}]}},{"slug":"beat-greenhouse","entry":{"title":"Beat Greenhouse","thumbnail":{"url":"https://imgur.com/S0HBUqQ.png","alt":"Thumbnail for the Beet Greenhouse project; two plants connected to the combining machine by some brightly colored tubes."},"contents":[{"type":"paragraph","text":"The Beat Greenhouse is a playful exploration of music as a cultivation experience. I created it in the spring of 2022 as my master’s thesis project. The goal of the project was to experience music creation as if it were a garden that you were cultivating, selectively breeding for desirable traits until you get a plant - or song - that you enjoy."},{"type":"paragraph","text":"Beat Greenhouse is a game created using web technologies! You can play it for yourself at the link below. All you need is a few MIDI files, and you’re ready to go!"},{"type":"link","text":"Play online at beatgreenhou.se","url":"https://beatgreenhou.se"},{"type":"link","text":"View the source code on GitHub","url":"https://github.com/peterrosenthal/beat-greenhouse"},{"type":"video","url":"https://www.youtube.com/embed/FsaLBjyGUrU?si=UXM226GdrFolYWg8"},{"type":"image","url":"https://imgur.com/CVgO6Gs.png","alt":"Two plants connected to the combining machine by some brightly colored tubes."},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse/raw/main/process/new-plants-look.png","alt":"Some very pretty plants from the Beat Greenhouse."},{"type":"image","url":"https://imgur.com/Dkuho8f.png","alt":"A top down view of a video game greenhouse full of plants."},{"type":"subheading","text":"Plantsongs Part 1 - The Plants"},{"type":"paragraph","text":"The Beat Greenhouse is a greenhouse that grows plantsongs; something that is both a plant and a song. In order to create generative plantsongs, first I had to create generative plants. I had worked on creating generative plants multiple other times on smaller projects prior to this one, and I had done plenty of research. I knew the next plant-generating algorithm I wanted to try creating was one based on a paper I had found, “Modeling Trees with a Space Colonization Algorithm” by Adam Runions et al."},{"type":"paragraph","text":"In this plant-generating algorithm, a 3D envelope of space is randomly populated with attraction points. The plant then grows from the bottom towards these attraction points, branching off whenever multiple attraction points enter a certain radius of the plant. With connecting the plants to songs in mind, I modified the algorithm to insert as many dimensions of control as possible, such as the size and shape of the envelope, or the skew(s) of the “randomness” of the attraction points, or how attractive they are, etc. And then finally, I visualized these plant forms into more realistic looking trunks and branches by stacking conical segments on top of each other to create a tapered look, and then adding leaves to the branch tips."},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse/raw/main/process/plant-envelope-nodes-and-attractors.png","alt":"Generative branching from the algorithm described above. On the right are a bunch of controls to change the results."},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse/raw/main/process/nice-plant-nodes.png","alt":"An early example of me working on the plant generator algorithm."},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse/raw/main/process/plant-with-leaves.png","alt":"A plant generated by the plant algorithm with better visualization techniques."},{"type":"subheading","text":"Plantsongs Part 2 - The Songs"},{"type":"paragraph","text":"I say “part 2” only because song is the second word in the portmanteau that is plantsong, but the songs part of the equation actually came first, and was the entire inspiration behind the project. When I discovered Magenta in 2020, a team at Google focused on machine learning in music, I was inspired by one of their recent (at the time) projects called MusicVAE. MusicVAE is a variational autoencoder, which is quite different, but also somewhat similar at the same time, to the large language models that are popular today. One of its features is to interpolate between two given MIDI songs. I saw this, combined with a little bit of randomness, as a way to “breed” desirable songs together over generations, just like the process of selective breeding or cultivation for plants. And thus the inspiration for the project was born."},{"type":"paragraph","text":"I actually started playing with creating the “breeding” algorithm, using MusicVAE as well as some of my own tools, back in the fall of 2020 as a project for another class during the first semester of my master’s program. So I had plenty of time to refine it and then hook it into my plant generating algorithm."},{"type":"paragraph","text":"The key to connecting the plants and the songs together in order to create plantsongs was the VAE part of MusicVAE. The variational autoencoder was able to take any MIDI song and encode it down into a single 256-dimensional vector. So all I needed to do was create 256 inputs into the plant generating algorithm, and each song is uniquely tied to a plant and backwards as well!"},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse-proto/raw/milestone-1/res/magenta-prototype-2020-12-07.png","alt":"A screenshot of the beat-breeding prototype where two MIDI songs are designated as parents and can be bred together to reveal a child."},{"type":"subheading","text":"Gameplay"},{"type":"paragraph","text":"In order to make the experience more immersive, I wanted to make it like a game where you were a greenhouse worker working with these plantsongs, hence the name Beet Greenhouse. I have experience with game development on multiple platforms, but because Magenta.js was a web-based library, I wanted to create the whole game on the web. So I created a first person game in THREE.js, and 3D modeled a greenhouse to be the surroundings, as well as some benches for the plants to rest on. I integrated instructions into the pause menu to explain the difference between the “workbenches” which will be refreshed every generation, and the “showbenches” which will keep plants around between generations."},{"type":"paragraph","text":"I wanted every action to be taken in-game. That meant that every setting of the algorithm that the user could control was a dial or a knob somewhere in the greenhouse that the user would click and drag to set. Instead of searching around in some settings menu, the user (player) was really engaged in the actions they were taking. This also meant that I 3D modeled “machines” for the player to take actions like combining/breeding together plantsongs, playing their song outloud, and importing a MIDI song into the game to become a plantsong."},{"type":"image","url":"https://imgur.com/gnonSJl.png","alt":"A menu showing how to upload a MIDI file to the beat greenhouse."},{"type":"image","url":"https://imgur.com/wPoM15k.png","alt":"A box with a plate on top and a play button on the front. This is the interpreter machine, which will play a plant as a song."},{"type":"image","url":"https://imgur.com/WGU43IA.png","alt":"A table with a tablet to interact with, and a crazy looking contraption to generate plants with."},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse/raw/main/process/first-machines-models-in-game.png","alt":"A screenshot from Beat Greenhouse showing all the main machines next to each other."},{"type":"subheading","text":"Custom Graphics and GLSL"},{"type":"paragraph","text":"In order to make the game look exactly how I wanted it to look, I needed to create some custom shaders for post-processing and rendering. One visual aesthetic I was very keen on was outlining everything. I was creating toon-ish (though not fully toon) shaders for everything so far, but it just wasn’t popping without that borderlands style outline that I had in my head. I made that happen by creating an edge-detection shader, or rather pipeline of shaders. First it renders the scene in regular color, and detects edges based on dramatic differences in color there. Then it renders the scene in a depth rendering, and detects edges likewise on that rendering. Third, it renders the scene with a normal rendering, where each face of each object is colored based on its normal direction, and detects the edges likewise. And finally to save on rendering the scene a fourth time, we use the original color rendering and apply a weighted average of all the edges detected on top of it as a solid black to give a satisfying and unique outline to every object."},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse/raw/main/process/depth-render.png","alt":"Depth render; a black-and white view of the beat greenhouse where closer things are white and farther things are black."},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse/raw/main/process/normals-render.png","alt":"Normals render; the same viewpoint as before, but everything is colorful; each face of each object is assigned a different color based on its normal direction."},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse/raw/main/process/outline-contributions.png","alt":"Edge Detect render; the same viewpoint, but everything is black, and the edges of objects are colorful."},{"type":"image","url":"https://github.com/peterrosenthal/beat-greenhouse/raw/main/process/first-attempt-at-outline-effect.png","alt":"Outline render; the same viewpoint, but everything is rendered normally except for the edges that were detected before, which are rendered black. This is the final effect."},{"type":"subheading","text":"Tools"},{"type":"list","entries":["TypeScript","THREE.js","Magenta.js","Blender","GLSL","Python","Google Cloud Functions"]},{"type":"subheading","text":"Process"},{"type":"link","text":"Check out my extensive process blog here!","url":"https://github.com/peterrosenthal/beat-greenhouse/blob/main/process/README.md"}]}},{"slug":"tilesish","entry":{"title":"Tilesish","thumbnail":{"url":"https://imgur.com/iZhWi1N.png","alt":"Thumbnail for the Tilesish project; a blue circle is layered over a green triangle, which in turn is layered over a red square."},"contents":[{"type":"paragraph","text":"Tilesish is a game I created in the fall of 2021 for one of my ATLAS classes, “Game Audio,” during my master’s. While roughly inspired by the game “Tiles” by NYT Games, it also very much has its own flair to it. The game consists of 16 tiles, each with 3 layers of a particular shape and color. The player clicks on tiles to match them up based on matching top-layer shape and color, which then removes that shape from the selected tiles. The player then gets more points as they combo their layer removals together. Because the player can combo an odd number of tiles together, it’s on the player to play the hand that was dealt to them the best. Sometimes, if you don’t play  the hand right, you won’t actually get to clear every tile. Clearing every tile is so rare that in fact, it gives you a massive bonus in points. Don’t be discouraged if you can’t clear every tile though, you can still keep shooting for a new high score!"},{"type":"link","text":"Play Tilesish at tilesish.vercel.app","url":"https://tilesish.vercel.app/"},{"type":"link","text":"View the source code on GitHub","url":"https://github.com/peterrosenthal/ctd-game-audio/tree/main/project-1/src"},{"type":"image","url":"https://imgur.com/rLGrgd4.png","alt":"A screenshot of the Tilesish game, 16 tiles are filled with colorful shapes, ready to be clicked on."},{"type":"subheading","text":"Tools"},{"type":"list","entries":["JavaScript","Pts.js","Tone.js","Reaper DAW"]}]}},{"slug":"oscinoodles","entry":{"title":"Oscinoodles","thumbnail":{"url":"https://imgur.com/WJfoffE.png","alt":"Thumbnail for the Oscinoodles project; three colorful noodles sway back and forth."},"contents":[{"type":"paragraph","text":"Oscinoodles is a game I created in the fall of 2021 for one of my ATLAS classes, “Game Audio,” during my master’s. Oscinoodles is a playful/game-like music creation experience in which the player pulls noodles out of the ground, and flicks them back and forth to give them an oscillating motion. Every time the noodle swings back and forth, it sings out a note based on how long it is (the bigger the noodle, the deeper the note). Because of the spatial audio aspect of the game, the player can create a whole symphony of Oscinoodles, and then run away to a quieter place and start creating a new one."},{"type":"link","text":"Play online at oscinoodles.vercel.app","url":"https://oscinoodles.vercel.app/"},{"type":"link","text":"View the source code on GitHub","url":"https://github.com/peterrosenthal/ctd-game-audio/blob/main/project-2/"},{"type":"image","url":"https://github.com/peterrosenthal/ctd-game-audio/blob/main/project-2/process/noodles-with-faces.gif?raw=true","alt":"A gif showing three noodles bouncing back and forth with cute faces on top and their mouths open when they sing."},{"type":"subheading","text":"Tools"},{"type":"list","entries":["TypeScript","THREE.js","Reaper DAW"]},{"type":"subheading","text":"Process"},{"type":"link","text":"Read my in depth process blog here","url":"https://github.com/peterrosenthal/ctd-game-audio/blob/main/project-2/README.md"}]}},{"slug":"carbon-cost","entry":{"title":"The Carbon Cost of Cryptocurrency","thumbnail":{"url":"https://imgur.com/JQHFVXN.png","alt":"Thumbnail for the Carbon Cost of Crypto project; a bunch of city buildings piled on top of each other, all contained within a box."},"contents":[{"type":"paragraph","text":"The Carbon Cost of Cryptocurrency is a project that I created in the fall of 2021 for an ATLAS class “Front-End Web Development” during my master’s. The goal of the project was to communicate what the carbon cost of each cryptocurrency was at the time, and hopefully have an emotional impact encouraging users to use environmentally friendly cryptocurrencies (or even better: not use crypto at all)."},{"type":"link","text":"Check out the the project at carbon-cost-of-crypto.vercel.app","url":"https://carbon-cost-of-crypto.vercel.app/"},{"type":"link","text":"View the source code on GitHub","url":"https://github.com/peterrosenthal/ctd-fwd/tree/main/code/project-1"},{"type":"paragraph","text":"The user can choose between 3 different crypto currencies to visualize the carbon footprint of: BitCoin, Etherium, and Tezos. They can also choose between 3 different objects to measure that carbon footprint by: a lump of coal, a car, and a city. The lump of coal represents the equivalent carbon emissions of burning an entire ton of coal. That means if those lumps are falling every couple of seconds, that crypto currency network is emitting the equivalent amount of an entire ton of coal burned, just every few seconds. The car represents the equivalent carbon emissions as driving a car on a highway for 10 hours straight. I think you’ll be surprised just how quickly those ones fall from the sky! And finally the city represents the equivalent carbon emissions as powering an entire Denver-sized city for a whole minute (so if those are dropping quicker than once a minute, then that crypto network is using more power than a Denver-sized city)."},{"type":"image","url":"https://imgur.com/anXPfWf.png","alt":"A box full of coal. Each lump of coal represents an entire ton of coal burned by the crypto network."},{"type":"image","url":"https://imgur.com/Vm5ZAC9.png","alt":"A box full of cars. Each car represents a crypto network emitting the same amount of CO2 as driving a car for 10 hours straight."},{"type":"image","url":"https://imgur.com/2bkhGwq.png","alt":"A box full of cities. Each city represents a crypto network emitting the same amount of CO2 as providing power to an entire Denver-sized city for a whole minute."},{"type":"subheading","text":"Tools"},{"type":"list","entries":["TypeScript","THREE.js","Cannon-ES physics engine","Blender"]},{"type":"subheading","text":"Process"},{"type":"link","text":"Check out my extensive process blog here!","url":"https://github.com/peterrosenthal/ctd-fwd/blob/main/process/project-1/README.md"}]}},{"slug":"sonatome-teller","entry":{"title":"Sonatome Teller","thumbnail":{"url":"https://imgur.com/JKjALtH.png","alt":"Thumbnail for the Sonatome Teller project; an antique looking wooden trapezoidal prism shaped stereo with a lid style opening on the top."},"contents":[{"type":"paragraph","text":"The Sonatome Teller is a piece of speculative fiction created in the spring of 2021 by Xavier Corr, Bella Colosimo, and me. When the Sonatome Teller tells a story, you are transported to the uncertainly dystopian future: The Provinces of Yigdrasil, a self-sustaining ecosystem meant to shelter humanity from an inevitable nuclear fallout. Following a period of prolonged global warfare, various political and financial powers established contingency plans in order to ensure the survival of the human race. The Amazon funded World Tree was one such plan. Over time, the inhabitants of the World Tree went on to generate their own customs and culture, involving a division of 5 provinces, focused on maintaining various aspects of society."},{"type":"paragraph","text":"Residents of each of the provinces used the Sonatome Teller to record stories of their everyday lives, or poems, or other art that was meaningful to them. They would record these stories onto Sonatomes."},{"type":"paragraph","text":"To use the Sonatome Teller, lift the lid, pick up a Sonatome, and place it inside the lid. Close the lid, and then the audio encoded into the Sonatome will then play out of the Sonatome Teller."},{"type":"image","url":"https://imgur.com/0Be84AO.png","alt":"An antique looking wooden trapezoidal prism shaped stereo with a lid style opening on the top. This is the sonatome."},{"type":"image","url":"https://imgur.com/UjT3y6m.png","alt":"The same box as earlier, but with its lid open, revealing a fancy colorful resin core, with an indentation, clearly meant for something to be placed there."},{"type":"image","url":"https://imgur.com/CnJQMYf.png","alt":"A bunch of tomes laid out neatly on the table."},{"type":"image","url":"https://imgur.com/XNqdL0Q.png","alt":"One of the tomes is picked up."},{"type":"image","url":"https://imgur.com/NTz8lo0.png","alt":"The tome is placed into the indentation in the sonatome. A story begins to play."},{"type":"video","url":"https://player.vimeo.com/video/543849925?h=40ae7df12e&badge=0&autopause=0&player_id=0&app_id=58479&wmode=opaque"},{"type":"subheading","text":"Tools"},{"type":"list","entries":["Raspberry Pi","Python","Resin casting and dyeing","3D printing","NFC"]},{"type":"subheading","text":"Source Code"},{"type":"link","text":"View the source code for the Sonatome Teller on GitHub","url":"https://github.com/peterrosenthal/ctd-design-studio"}]}},{"slug":"flocking","entry":{"title":"Flocking: Interactive Graphing","thumbnail":{"url":"https://imgur.com/kDYeI4x.png","alt":"Thumbnail for the Flocking - Interactive Graphing project; a flock of arrow shaped boids (bird-oids) flock around."},"contents":[{"type":"paragraph","text":"Flocking - Interactive Graphing is a project I created in the spring of 2021 for a class “CSCI 5314 - Dynamic Models in Biology” during my master’s. Behind it all is a numerical analysis type simulation of the “boids” flocking algorithm. That simulation can be controlled in a simple manner with a straightforward play/pause button, as well as a speed controller that allows running as slow as 1/64th time, and as fast as your computer will handle. In addition to the simple controls, there is a much more powerful “run settings” which features a plain-language way to program useful scientific strategies into the simulation, such as run repetition, data averaging, and parameter sweeps. The output of the simulation will always be seen in the main flock visualizer, but then the user can also add as many additional graph outputs as they want, where they can create 2D plots by simply choosing what data is represented on the X-axis and what data is represented on the Y-axis."},{"type":"link","text":"Check out the project at flocking-interactive-graphing.vercel.app","url":"https://flocking-interactive-graphing.vercel.app/"},{"type":"link","text":"View the source code on GitHub","url":"https://github.com/peterrosenthal/csci-dynamic-models-in-biology/tree/main/FinalProject"},{"type":"image","url":"https://imgur.com/Owr5rZJ.png","alt":"A screenshot from the Flocking Application, showing all the main features: the run bar, run settings, flock visualizer, and an auto-plot."},{"type":"image","url":"https://imgur.com/kDYeI4x.png","alt":"A screenshot of the main flock visualizer. Triangle shaped “boids” fly around in a square."},{"type":"image","url":"https://imgur.com/2h5GKgM.png","alt":"A screenshot of the run settings. Multiple drop-down menus and input fields chain together to create a plain-language programming language for scientific strategies such as parameter sweeps to be implemented."},{"type":"image","url":"https://imgur.com/7BdSdcO.png","alt":"A screenshot of the automatically generated plot based on the user’s choice of X-axis data and Y-axis data. This plot happens to be of “Time vs Radius of Gyration”, for a parameter sweep over the repulsion factor, that shows a direct relationship between radius of gyration and repulsion factor."},{"type":"video","url":"https://www.youtube.com/embed/x0YUBJcM8TY"},{"type":"subheading","text":"Tools"},{"type":"list","entries":["TypeScript","P5.js","THREE.js"]},{"type":"subheading","text":"Source Code"}]}},{"slug":"minds-garden","entry":{"title":"Mind’s Garden","thumbnail":{"url":"https://i.imgur.com/EBNoi6x.png","alt":"Thumbnail for the Mind’s Garden project, some majestic and colorful trees sit on a hillside, and the sky is filled with geometric star fractals."},"contents":[{"type":"paragraph","text":"Mind’s Garden is a musical exploration game and immersive audio visualizer. The game, which takes place in a garden within your mind, allows you to import any song you want to play alongside. However, the song isn’t just an accessory to the game; it is the game! The song is broken into pieces and scattered throughout your “head”, and it is your task to find them and unite the song. As you search for the pieces of the song, don’t forget to take a look at the garden around you, and watch the plants as they interact with the music that’s playing. The game is free to download and play from our itch.io page!"},{"type":"link","text":"Play Mind’s Garden on itch.io","url":"https://jaristotle.itch.io/minds"},{"type":"link","text":"View the source code on GitHub","url":"https://github.com/peterrosenthal/minds-garden"},{"type":"image","url":"https://i.imgur.com/k24y7BW.png","alt":"A forest scene from Mind’s Garden with trees changing color to the beat and dancing to the music."},{"type":"image","url":"https://i.imgur.com/6Z1CpTq.png","alt":"A lush but potentially hostile jungle scene from the Mind’s Garden game. Enjoy the music, but watch out for the carnivorous plants!"},{"type":"image","url":"https://i.imgur.com/ch5ADL2.png","alt":"A desert scene found in the Mind’s Garden game. The various cacti of different sizes and shapes jump and dance to the music."},{"type":"image","url":"https://i.imgur.com/Gv2waD3.png","alt":"An ocean scene found in the Mind’s Garden game. The ground is filled with coral and fish fly through the water around and above you."},{"type":"video","url":"https://www.youtube.com/embed/8VasHc0pNV0"},{"type":"subheading","text":"Process"},{"type":"paragraph","text":"I created Mind’s Garden with two of my classmates from the ATLAS Institute at CU Boulder. Jared Myers, Kara Metcalfe, and it was our undergraduate capstone project. We were given a 12 week long creation phase during the semester, and kept a weekly development blog the whole time! That development blog, or process log, can be found over on our GitHub repository for the project, which also contains all of the source code too!"},{"type":"link","text":"Mind’s Garden on GitHub","url":"https://github.com/peterrosenthal/minds-garden/tree/master/Process"},{"type":"paragraph","text":"Kara focused mostly on creating all of the visual assets for the game like 3D models and shaders. Jared was the gameplay programmer, coding things such as the asset and objective placement around the map. For my part: I did a lot of programming too, but I focused more on the systems programming. I created things such as the game’s procedural terrain generation algorithm, as well as systems to manage all of our 800+ different colors and determine how those colors interacted with  songs. I was also responsible for creating the different builds of the game for different operating systems as I had the most access and experience with multiple operating systems other than Microsoft Windows. And to give my artistic skills a test, I designed and drew our logo for the game."},{"type":"image","url":"https://i.imgur.com/8wQw3sA.png","alt":"The logo which I crafted for the Mind’s Garden game!"},{"type":"paragraph","text":"One of my main goals throughout the project was to always code with my teammates in mind. That meant that instead of just designing and developing “my parts” of the game, I’d create and share tools with my teammates so they could give direct hands-on input on my designs. For example, I didn’t just create a piece of procedurally generated terrain, I created a tool to create a piece of procedurally generated terrain, and opened it up to my entire team to work together on designing the ground the player walks on. If you want to read more about all the million other small things we created for the game, check out our complete weekly development blog!"},{"type":"image","url":"https://i.imgur.com/S8cJOla.png","alt":"A shot of the Mind’s Garden game during development."},{"type":"subheading","text":"Tools"},{"type":"list","entries":["The Unity Game engine","C# programming language with the JetBrains Rider IDE","Spleeter source stems separation library","GNU Image Manipulation Program"]},{"type":"subheading","text":"Reception"},{"type":"paragraph","text":"Unfortunately due to the COVID-19 pandemic starting right as we were graduating, we never got the opportunity to show off our capstone project in person. We were able to show people over zoom though, and send people the link to download the game themselves, and despite the lack of in person experience, most people were still super excited about our concept and really enjoyed exploring our interactive garden world. We had enough different color schemes to choose from that there was something for everybody to love there too. And our simple controls made the game super easy to give to people of all ages; kids absolutely loved our game. However, our 12 week development time felt very cramped for such an ambitious project, and we didn’t get the chance to iron out every single bug. So while most people are able to play the game without ever encountering a bug, we’ve gotten too many reports of people not even being able to make it past the main menu to consider the game “bug free.” Even though the capstone project is long finished, I still hope to one day come back though and squash those last few remaining bugs."}]}},{"slug":"gosutoraito","entry":{"title":"Gosutoraito","thumbnail":{"url":"https://i.imgur.com/KFvPlNM.jpg","alt":"Thumbnail for the Gosutoraito project, shows a small wooden temple in a snowy hillside."},"contents":[{"type":"paragraph","text":"Gosutoraito is a first person atmospheric puzzle game set in a ghostly temple. The game is supposed to symbolically represent a samurai’s transition to the afterlife, encountering different ghosts of his past in each level. Every puzzle in the game is based on the same mechanic of guiding a beam of light from room to room, reflecting it around corners with mirrors that the player rotates. As the levels get harder, more complicated mechanics such as splitting the light beam in two are introduced. Download the game for free from our itch.io page for it!"},{"type":"link","url":"https://karasel.itch.io/gosutoraito","text":"Gosutoraito on itch.io"},{"type":"image","url":"https://i.imgur.com/hB3oT9l.jpg","alt":"At the start of the game, a small wooden temple sits in a snowy forest."},{"type":"subheading","text":"Process"},{"type":"paragraph","text":"For my final project in the class ATLS 4140 - Game Development, I teamed up with two of my classmates, Jared Myers and Kara Metcalfe, to create Gosutoraito. The game was created over the course of 6-8 weeks, which is not much time, and yet it was the biggest project and longest any of us had dedicated yet to working on a video game at the time. Kara worked on most of the visual assets of the game, like 3D models and environmental effects, and Jared did all of the level design and any coding/development that went with that. I wanted to work on my skills in both areas. Most of my time working on this project was dedicated to programming things such as our unique player controls and our reflection math, but I also spent some time creating a 3D model of a ghost and improving small graphical things in the game, like the look of the light beam, something central to every level."},{"type":"image","url":"https://i.imgur.com/J4L9Jub.jpg","alt":"A beam of light shines down from the ceiling of the temple into the puzzles."},{"type":"image","url":"https://i.imgur.com/Xdmj1fN.jpg","alt":"The beam of light is reflected throughout the puzzle on mirrors."},{"type":"paragraph","text":"My team created a process blog for this project that you can read on the project’s GitHub repository for more insight into our design and development process for the game!"},{"type":"link","url":"https://github.com/peterrosenthal/gosutoraito/tree/master/process","text":"Gosutoratio on GitHub"},{"type":"subheading","text":"Tools"},{"type":"list","entries":["The Unity Game Engine","C# and JetBrains Rider","Autodesk Maya","Adobe Photoshop"]},{"type":"subheading","text":"Reception"},{"type":"paragraph","text":"“That’s kind of racist.” -Zhenghua Yang, Founder and CEO of Serenity Forge"},{"type":"paragraph","text":"He was referring specifically to the hat we put on one of the ghosts to indicate that they were a farmer before being trapped in the afterlife. We went through after the final presentation to remove that hat, but upon further reflection, those words really do go beyond just a hat. We were 3 white kids trying to make a game with heavy Eastern spiritual themes. The name was literally just Google Translate to Japanese for “ghost light,” and none of us really had enough cultural knowledge to make anything else about the game legitimate, so the spiritual themes were pretty much just whatever we wanted. And that… that’s kind of racist."},{"type":"image","url":"https://i.imgur.com/0xTAhJ0.jpg","alt":"Two ghosts chase the player down as they try to solve the puzzles. One of the ghosts is wearing a farmer’s hat."},{"type":"paragraph","text":"Other than the cultural problems of the game, it turned out technically awesome. The game had a good learning and difficulty curve, and the final few levels were surprisingly more challenging than any of us had anticipated. The light reflection mechanic turned out to be an inspiration in the master’s thesis project of Maria Deslis, a fellow ATLAS student at the time. That project, Eternal Eclipse, also borrowed the control scheme I developed, since as a team we decided to open source our game!"}]}},{"slug":"tropical-cyclones","entry":{"title":"Tilted Tropical Cyclones","thumbnail":{"url":"https://i.imgur.com/a1YPK6G.png","alt":"An overhead view of the clouds from a simulated hurricane."},"contents":[{"type":"paragraph","text":"In the summer of 2019, I had the incredible opportunity to work with researcher David Schecter at NorthWest Research Associates. We were investigating how different factors affect the development time from a tropical cyclone to a hurricane, primarily the tilt factor of the hurricane. In tropical cyclone terms, tilt is simply just the horizontal difference between the center of the cyclone at the bottom and the center of the cyclone at its top. As part of this research, I got to run the code on a supercomputer many orders of magnitude more powerful than my own computer, which I found particularly exciting."},{"type":"image","url":"https://i.imgur.com/4irMJ9U.png","alt":"An overhead view of the clouds from a simulated hurricane."},{"type":"subheading","text":"3D Visualization"},{"type":"paragraph","text":"Over the course of the summer, I ran dozens of simulations and collected 10TB of data. I prepared it all for standard analysis, but was also given the freedom to experiment and explore my own interests and their intersections with this work. I’m a creative technologist, not an atmospheric scientist, and as fun as it was to collect 10TB of data, all of it was just ones and zeros. We graphed the data, but you needed to read at least the first 5 chapters of any atmospheric science textbook before properly understanding any of the 2 dimensional graphs that are standard to include in a research paper of that type. Not many people outside of atmospheric sciences have read the first 5 chapters of an atmospheric science textbook, so I was disappointed with how hard it was to share the work I was doing with everyone else I knew."},{"type":"image","url":"https://i.imgur.com/6sJ0Qjr.png","alt":"An isosurface shows the “tilt”: horizontal separation of the cyclone on top vs. bottom."},{"type":"paragraph","text":"Early on in the research, I came across Leigh Orf’s research on tornadoes that was done using CM1, the same numerical model we were using to run our simulations. These tornadoes were particularly interesting to me not only due to the shared model, but because Leigh Orf collaborated with David Bock to create physically based renderings of the tornado. Those 3D renderings were incredible because they just showed the clouds that would be there if the simulated tornado was real. There was absolutely no prerequisite need for atmospheric science knowledge to read them; it passed the eyeball test and looks like a tornado to everybody."},{"type":"link","text":"Leigh Orf and David Bock’s tornado simulations and renders","url":"http://lantern.ncsa.illinois.edu/Vis/XSEDE/XSEDE15/Bock_Leigh.mov"},{"type":"paragraph","text":"I wanted to replicate this eyeball testability of the data by creating 3D cloud renderings of my own with the data I was creating. After trying out several different rendering and visualization softwares, I settled on Mayavi2 by EnThought. It is both a visualization program written in Python and a visualization library written for Python, which enabled me to easily integrate it into our already existing data analysis/visualization workflow that consisted mainly of Python and Jupyter notebooks. In addition to rendering videos and stills of the clouds in our hurricanes like I was originally inspired to do, I also experimented with creatively visualizing different scientific qualities of the simulation, like the vorticity, or how much the wind was turning direction at each point. Check out the renderings in the pictures accompanying this post!"},{"type":"image","url":"https://i.imgur.com/jVDQ6Wl.png","alt":"A birds eye view shows the clouds from a simulated hurricane at an angle, and the ground is painted with the wind speeds at the location."},{"type":"image","url":"https://i.imgur.com/KtPW4Gp.png","alt":"A side-on view of the simulated hurricane. Using a combination of isosurfaces and volume rendering of the vertical vorticity, the powerful “eye” of a fully developed hurricane is visualized as well as smaller vortical dipoles."},{"type":"subheading","text":"Animated Explainer Video"},{"type":"paragraph","text":"Real-life hurricanes are often not perfectly aligned from top to bottom, usually caused by factors such as wind blowing in different directions at different altitudes (this is pretty much always happening in the air above you to some extent). This not-perfect alignment is what we called the hurricane’s tilt. It’s not too complicated of an idea, but whenever someone asked what I was working on, and I replied, “tilted tropical cyclones,” I would always be met with blank stares and “umm, what?”s. So for my final project in ATLS 3110 - Motion, an animation design class, I created an explainer video dedicated to answering the question “what is a tilted tropical cyclone?”"},{"type":"paragraph","text":"The explainer video was scripted and recorded by me, and animated in Adobe AfterEffects with illustrations I created in Adobe Illustrator. It was met with a lot of positive feedback that it was doing its job of explaining tilted tropical cyclones really well. I hope it can help you understand the topic better too! I originally posted it to Vimeo, but have recently moved it to YouTube in an effort to consolidate my video content from ATLAS."},{"type":"video","url":"https://www.youtube.com/embed/KQbJA5Mnc88"},{"type":"link","text":"Titled Tropical Cyclones Explained on YouTube","url":"https://www.youtube.com/watch?v=KQbJA5Mnc88"},{"type":"subheading","text":"Tools"},{"type":"list","entries":["NCAR CISL’s Cheyenne supercomputer","CM-1 cloud resolving numerical model","Python: numpy, pyplot, and Jupyter notebooks","Mayavi2 3d data visualization library","Adobe Illustrator and AfterEffects"]}]}},{"slug":"pn-barrier","entry":{"title":"Breaking the Peierls-Nabarro Energy Barrier","thumbnail":{"url":"https://i.imgur.com/PqXk3fo.png","alt":"A 3D scientific plot shows a wave traveling forward with a little bit of dispersion as time increases."},"contents":[{"type":"paragraph","text":"The first research project I worked on was in the Applied Mathematics department at CU Boulder. I worked as a research assistant with Professor Mark Ablowitz and Justin Cole when he was still a postdoctoral researcher at CU Boulder. The paper investigates how topological invariance interacts with the Peierls-Nabarro energy barrier, two forces that seem to be at odds in a topological insulator system. For the research, I modified and ran simulation code in MATLAB written by Justin Cole, managed and analyzed the data, and assisted in the writing of the paper."},{"type":"link","text":"Read “Peierls-Nabarro barrier effect in nonlinear Floquet topological insulators” here","url":"https://doi.org/10.1103/physreve.103.042214"},{"type":"link","text":"Professor Mark Ablowitz’s website","url":"https://sites.google.com/site/ablowitz/"},{"type":"link","text":"Professor Justin Cole’s Website","url":"https://sites.google.com/view/jtcole"},{"type":"subheading","text":"3D Interactive Visualization"},{"type":"video","url":"https://www.youtube.com/embed/vcgMK3EQsik"},{"type":"paragraph","text":"During my second research position, the Tilted Tropical Cyclones project, I had the chance to explore the field of creative data visualization. For school projects I had gotten significantly comfortable with using game engines. So recently I decided to revisit the data from this project to be used in my first adventure into combining science and games."},{"type":"paragraph","text":"Just like the Tilted Tropical Cyclones visualization, I wanted the visualization to represent the actual physical experiment that we were numerically simulating, so I named this sub-project the Honey Visualizer after the honeycomb lattice structure of the experiment. In physical experiments of the type of topological insulators we’re simulating, waveguides are etched into a block of acrylic in a honeycomb pattern. These waveguides act like fiber optic cables when light is shone into them. A highly localized wave of light -- meaning very bright in one waveguide, and much much less bright in the surrounding waveguides -- is shone into one end of the acrylic block. As the wave of light travels down the block of acrylic, the center of the wave jumps up one waveguide, and then another waveguide, and it keeps moving from waveguide to waveguide in one direction only; it never stops or changes direction."},{"type":"paragraph","text":"I tried my hardest to recreate the visual look of doing that experiment in real life via the Godot game engine. I used the data we got from the simulations, and the first prototype turned out really good. I hope to have the time in the future to add more robust data-set selection and other features to the visualizer to take it beyond the prototype stage!"},{"type":"subheading","text":"Tools"},{"type":"list","entries":["MATLAB","LaTeX","Adobe Illustrator","Godot Game Engine"]}]}}]